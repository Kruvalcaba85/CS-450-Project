{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S.a.M: Song and Music Recommendation System \n",
    "\n",
    "Hi! My name is S.a.M. I'm an AI recommendation system that allows the user to input their favorite movie or song and get recommendations based off it. This is posible through the use of content based filtering, principal component analysis, k-means clustering, cosine similartiy and eueclidean distance. I'm split up into 3 main compoenents, there's a msuic feature, a movie feature, and an interactive menu. Let's begin with the music componenet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Loading the data\n",
    "In order to start, we will have to import the necessary libraries and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now read the CSV file containing our dataset and put it in a pandas dataframe. In dataframe format, we will able to show important information such as the format of the dataset and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\kylek\\Downloads\\CS 450 Project\\data\\dataset.csv\", index_col=0) #index_col=0 used to removed any unnamed columns which our dataset had one\n",
    "df.head(10) #the head() function usually prints out the first 5 rows of the dataset but we will show the first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dimensions of our dataset are:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, our dataset consists of 114,000 songs with 20 features. These features consists of basic information of the song such as track name, artist, and album. But, it also has more interesting features that might be useful for use such as, loudness, acousticness, tempo, and genre. These can be useful when it comes to creating clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Cleaning the Data\n",
    "Before implementing clusters, we first need to clean our data of unnecessary columns. Clustering works best if we keep features that are numerical values. This will make our recommendation system more interesting as we will be recommending songs that sound similar rather than something simple such as genre. To achieve this, we will drop columns that do not contain numerical data. We will also be dropping NaN columns which are columns that contain missing or incomplete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True) #we will first drop any NaN values for efficiency\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have to manually drop any remaining uneccessary columns such as 'track_id', 'explicit', and 'track_genre'\n",
    "dropped_df = df.drop(['track_id', 'artists','album_name', 'track_name','explicit', 'track_genre'], axis=1)\n",
    "dropped_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New dimensions of dataset:\", dropped_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step before implementing clusters is to normalize our data. If we did not do that, our graph would be skewed to favored larger values such as duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = dropped_df.copy()\n",
    "final_df = StandardScaler().fit_transform(final_df)\n",
    "final_df = pd.DataFrame(final_df, columns=dropped_df.columns)\n",
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Implementing Principal Component Analysis\n",
    "For our clusters to be as efficient as possible, we will first reduce the dimensionality of our data to make our calculations more efficient. We will be doing this using Principal Component Analysis (PCA).\n",
    "\n",
    "PCA is a dimension reduction algorithm that reduces the dimensionality of a dataset while still maintaining crucial information. When combined with K-means clustering, it will lead to better defined clusters\n",
    "\n",
    "We begin by figuring out how many principal components we can reduce our data too. We do this by creating a PCA instance which will use as many components as the number of features in our data. We will then plot the cumulative explained variance ratio for each number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca_df = pca.fit_transform(final_df)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, we have 14 principal components since we have 14 features and each component has there own percentage of explained variance. We will plot the ratio and find at which number of components will we have explained variance ratio above 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_.cumsum()) + 1), pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--')\n",
    "plt.title('Explained variance by components')\n",
    "plt.xlabel('# of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our graph, at 9 principal components we will have cumulative explained variance ratio above 0.80 which means that will be the number of components we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=9)\n",
    "pca_df = pca.fit_transform(final_df)\n",
    "print(sum(pca.explained_variance_ratio_)) #to verify that 9 is the right number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Implementing K-means Clustering\n",
    "Step 4a. Elbow Method\n",
    "For K-means clustering to work, we need to figure out the k number of clusters for our data. This is a number that has to be figured out by us. It is important to have the right amount of clusters as having too little clusters leads to underfitting and have too many clusters leads to overfitting.\n",
    "\n",
    "To figure out how many clusters we will be using, we will be using the Elbow Method. This method involves creating a K-means clustering model with varying amounts of clusters, calculating the inertias of each and plotting it to see at which point do we see a elbow-like bend in the graph. So, we will be taking our Principal Component model and running it through the clustering algorithm to find the optimal amount of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "means = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, n_init=10)\n",
    "    kmeans.fit(pca_df)\n",
    "    means.append(i)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(means, inertias, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel(\"# of clusters\")\n",
    "plt.ylabel(\"Inertias\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick the point in which the inertia starts decreasing in a linear manner which appears to be point 3. We will be forming 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, n_init='auto')\n",
    "kmeans.fit(final_df)\n",
    "df['Cluster'] = kmeans.labels_ #assigns each song into their respective cluster\n",
    "final_df['Cluster'] = kmeans.labels_ #does the same into our modified dataframe\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will visualize our data. To do this well be plotting the first two principal components of our dataset while showing the cluster that each song is in. We will only be plotting two as it is not possible to plot 9 components but 2 is enough to showcase our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['pca_1'] = pca_df[:,0]\n",
    "final_df['pca_2'] = pca_df[:,1]\n",
    "plt.scatter(final_df['pca_1'], final_df['pca_2'], c=final_df['Cluster'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully clustered our songs! It is time to create our song recommending functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Create Song Recommender\n",
    "With our songs clustered, to create our recommender we need a way to locate song the user inputs in our database, find the cluster number and recommend songs inside that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Song_Index(track_name, df):\n",
    "    try:\n",
    "        track_index = df[df['track_name'] == track_name].index[0] #Finds the id of the first matching result\n",
    "        return track_index\n",
    "    except IndexError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(track_name, df):\n",
    "    track_index = get_Song_Index(track_name, df)\n",
    "    print('You chose: ' + track_name + ' by ' + df.loc[track_index]['artists'])\n",
    "    print('Here are your recommendations:')\n",
    "    cluster = df.loc[track_index]['Cluster'] #finds the cluster of the inputted song\n",
    "    filter = (df['Cluster'] == cluster)\n",
    "    \n",
    "    filtered_df = df[filter] #creates a dataframe with only songs of the indicated cluster\n",
    "    chosen_index = filtered_df.index.get_loc(track_index)\n",
    "    start_index = max(0, chosen_index - 5)\n",
    "    end_index = min(len(filtered_df), chosen_index + 6)\n",
    "    for i in range(start_index, end_index):\n",
    "        if i != chosen_index:\n",
    "            recommendation = filtered_df.iloc[i]\n",
    "            print(recommendation['track_name'] + ' by ' + recommendation['artists'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('Is This It', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the vast nature of our clusters, some of the recommendations may seem outlandish and only vaguely similar. To improve our recommender, we will introduce euclidean distance calculations into our recommender. We will be measuring the distance between our inputted song and every other song in the cluster and recommend the songs with the shortest distance to our song. The calculation will be based off certain features to make sure that our songs will be as close as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_euclidean_recommendations(track_name, features, df, final_df, n_songs=10):\n",
    "    \n",
    "    track_index = get_Song_Index(track_name, df)\n",
    "    print('You chose: ' + track_name + ' by ' + df.loc[track_index]['artists'])\n",
    "    print('Here are your recommendations:')\n",
    "    track_cluster = final_df.loc[track_index]['Cluster']\n",
    "    cluster = final_df[final_df['Cluster'] == track_cluster]\n",
    "    cluster_df = cluster[features]\n",
    "    \n",
    "    target_song = cluster_df.loc[track_index, features]\n",
    "    target_song = target_song.to_frame().T\n",
    "    \n",
    "    distances = euclidean_distances(target_song, cluster_df)\n",
    "    distances = distances.flatten()\n",
    "    sorted_indices = np.argsort(distances)\n",
    "        \n",
    "    most_similar_songs = cluster_df.iloc[sorted_indices[:n_songs + 1]]\n",
    "    indices = most_similar_songs.index\n",
    "    songs_in_df = df.loc[indices]\n",
    "    \n",
    "    for i in range(n_songs + 1):\n",
    "        if songs_in_df.iloc[i]['track_name'] != track_name:\n",
    "            print(songs_in_df.iloc[i]['track_name'] + ' by ' + songs_in_df.iloc[i]['artists'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['popularity', 'danceability', 'energy', 'loudness', 'speechiness', 'acousticness',\n",
    "           'instrumentalness', 'liveness', 'valence', 'tempo'] #the features we will be comparing\n",
    "get_euclidean_recommendations('Is This It', features, df, final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully implemented the music component of our recommendation system. Now let's move onto the movie portion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Loading data\n",
    " \n",
    "In order to start, we must import additonal libraries and resources that will be needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from ast import literal_eval\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a different dataset for our movie section so we have to read the new CSV files that contains our dataset and put it in a pandas dataframe. In dataframe format, we will be able to show important information such as the format of the dataset and its shape. Df1 contains movie_id, cast, and crew information. Df2 contains budget, genre, homepage, id, keywords, and other movie features. We will eventually combine the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r\"C:\\Users\\kylek\\Downloads\\CS 450 Project\\data\\tmdb_5000_credits.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our first dataset. Now let's take a look at our second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(r\"C:\\Users\\kylek\\Downloads\\CS 450 Project\\data\\tmdb_5000_movies.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets join the two datasets together and take a look at our new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns = ['id','tittle','cast','crew']\n",
    "movie_df = df2.merge(df1,on='id')\n",
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the shape our dataset. This will tell us how many movies and features are in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are returned a tupple, (4803, 23). This means that in our dataset, we have 4803 movies, each movie containing 23 features.\n",
    "\n",
    "We will begin our recommendation system with content based filtering. The content of the movie (overview, cast, crew, keyword, and tagline) are used to compute a similarity score with other movies. Then the movies with the highest similarity score are recommended. We'll start by computing the pairwise similarity scores for all movies based on their plot description. The plot description is given in the overview feature of our dataset.\n",
    "\n",
    "Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df['overview'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must perform data preprocessing in order to transform our raw data into something usable. We want to perform text processing in order to accurately analyze the plot description for each movie. This is achieved by computing the Term Frequency-Inverse Document Frequency (TF-IDF) vectors for each overview. Term frequency is the relative frequency of a word in a document. It's given as (term instance / total instance). Inverse Document Frequency is the relative count of documents containing the term and is given as log(number of documents / documents with term). The importance of each word to the document is equal to TF * IDF. This gives you a matrix where each column represents a word in the overview vocabulary and each row represents a movie. We perform these actions in order to reduce the importance of words that occur frequently in plot overviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#Replace NaN with an empty string\n",
    "movie_df['overview'] = movie_df['overview'].fillna('')\n",
    "\n",
    "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix = tfidf.fit_transform(movie_df['overview'])\n",
    "\n",
    "#Output the shape of tfidf_matrix\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's 20,978 words used to describe the 4803 movies in our dataset.\n",
    "\n",
    "Using this matrix, we can now compute a similarity score. There are multiple methods for finding a similary score such as eculidean, pearson and cosine similarty score. No method is better than another, they all have advantages for certain sceanrios. In this case, we will be using the cosine similarty score to calculate a numeric quanity that represent the similarity between two movies.\n",
    "\n",
    "Similarity = cos(θ) = (A * B) / ||A|| ||B|| (dot product)\n",
    "\n",
    "Since we have used the TF-IDF vectorizer, calculating the dot product will direclty give us the cosine similarity score. Therefore, we will use sklearn's linear_kernel()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a function that takes in a movie title as an input and outputs a list of the 10 most similar movies. We need a way to identify the index of a movie in our metadata DataFrame given its title. This can be achieved by reverse mapping the movies titles and DataFrame indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a reverse map of indices and movie titles\n",
    "indices = pd.Series(movie_df.index, index=movie_df['title']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Steps of Recommendation System\n",
    "\n",
    "1. Get the index of the movie based off title\n",
    "2. Compute cosine similarity scores for the particular movie with all movies. Then convert it into a list of tuples where the first element is its position and the second is the similarity score\n",
    "3. Sort the list of tuples based on the similarity scores\n",
    "4. Get the top 10 elements of this list. Ignore the first element since it's refering to itself\n",
    "5. Return the titles corresponding to the indices of the top elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that takes in movie title as input and outputs most similar movies\n",
    "def get_content_recommendations(title, cosine_sim=cosine_sim):\n",
    "    try:\n",
    "        # Get the index of the movie that matches the title\n",
    "        idx = indices[title]\n",
    "\n",
    "        # Get the pairwise similarity scores of all movies with that movie\n",
    "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "        # Sort the movies based on the similarity scores\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get the scores of the 10 most similar movies\n",
    "        sim_scores = sim_scores[1:11]\n",
    "\n",
    "        # Get the movie indices\n",
    "        movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "        # Create a DataFrame with the top 10 most similar movies and their similarity scores\n",
    "        result_df = pd.DataFrame(columns=['Title', 'Similarity Score'])\n",
    "        result_df['Title'] = movie_df['title'].iloc[movie_indices]\n",
    "        result_df['Similarity Score'] = [i[1] for i in sim_scores]\n",
    "\n",
    "        # Return the top 10 most similar movies\n",
    "        return result_df\n",
    "    except KeyError:\n",
    "        print(f\"Sorry, the movie '{title}' was not found in our database. Please check the title and try again.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out our content based recommendation by inputing our favorite movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_content_recommendations('Batman & Robin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our recommendation system does a good job at finding movies with similar plot descriptions but the quality of the recommendation could improve. For example, \"Batman & Robin\" returns all batman movies rather than movies with similar actors. We can improve our movie recommendation by building a recommendation based on 4 key features - movie plot keywords, the director, the 3 most popular actors, and the genre. In order to achieve this, we must manipulate our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credits, Genres and Keywords Based Recommender\n",
    "attributes = ['cast', 'crew', 'keywords', 'genres']\n",
    "for attribute in attributes:\n",
    "    movie_df[attribute] = movie_df[attribute].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll write functions that allow us to extract information from each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract director's name, return none if not found\n",
    "def extract_director(crew_data):\n",
    "    for crew_member in crew_data:\n",
    "        if crew_member['job'] == 'Director':\n",
    "            return crew_member['name']\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the top 3 elements from list \n",
    "def get_elements(elements):\n",
    "    if isinstance(elements, list):\n",
    "        names = [element['name'] for element in elements][:3]\n",
    "        return names\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define new director, cast, genres and keywords features that are in a suitable form.\n",
    "movie_df['director'] = movie_df['crew'].apply(extract_director)\n",
    "\n",
    "features = ['cast', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    movie_df[feature] = movie_df[feature].apply(get_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the new features of the first 3 films\n",
    "movie_df[['title', 'cast', 'director', 'keywords', 'genres']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must normalize our data. We have to convert the names nad keywords into lowercase and delete the spaces between them. This way our vectorizer doesn't count the Ryan of \"Ryan Reynolds\" and \"Ryan Gosling\" as the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data by converting strings to lowercase and removing spaces \n",
    "def normalize_data(data):\n",
    "    if isinstance(data, list):\n",
    "        return [str.lower(item.replace(\" \", \"\")) for item in data]\n",
    "    else:\n",
    "        #Check if director exists. If not, return empty string\n",
    "        if isinstance(data, str):\n",
    "            return str.lower(data.replace(\" \", \"\"))\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize all features\n",
    "features = ['cast', 'keywords', 'director', 'genres']\n",
    "for feature in features:\n",
    "    movie_df[feature] = movie_df[feature].apply(normalize_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our metadata soup (a string that contains all the metadata that we want to feed to our vectorizer (actors, director, and keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(features):\n",
    "    return ' '.join(features['keywords']) + ' ' + ' '.join(features['cast']) + ' ' + features['director'] + ' ' + ' '.join(features['genres'])\n",
    "movie_df['soup'] = movie_df.apply(create_soup, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is very similar to the text processing we pefromed with TF-IDF. This time we are going to utilize COuntVectorizer(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "word_matrix = vectorizer.fit_transform(movie_df['soup'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now created a word_matrix. We are able to use this word matrix to calcualte the cosine simialrty score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the cosine similartiy matrix \n",
    "cosine_similarity_matrix = cosine_similarity(word_matrix, word_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset index of our main DataFrame and construct reverse mapping as before\n",
    "movie_df = movie_df.reset_index()\n",
    "indices = pd.Series(movie_df.index, index=movie_df['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now reuse our get_recommendation() function by passing in the new cosine_similarity_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_content_recommendations('The Dark Knight Rises', cosine_similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented our song and movie compoenents we are able to create a user-friendly interactive menu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create functions that retrieve our recommendations based on user input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function that shows an interactive menu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! My name is S.a.M. I'm your Song and Movie recommendation system. I can't wait to get started.\n",
      "\n",
      "Please select one of the following options\n",
      "1. Get movie recommendations\n",
      "2. Get song recommendations\n",
      "3. Exit\n",
      "Please enter the song title you want a recommednation for\n",
      "You chose: Destroy Everything You Touch by Ladytron\n",
      "Here are your recommendations:\n",
      "I Know A Place by Jay Reatard\n",
      "Is This It by The Strokes\n",
      "空と虚 by sasanomaly\n",
      "空と虚 by sasanomaly\n",
      "Levántate y Anda by Avalanch\n",
      "Sun King - Remastered 2009 by The Beatles\n",
      "Mean Mr Mustard - Remastered 2009 by The Beatles\n",
      "Skiptracing by Mild High Club\n",
      "If I Could Find You (Eternity) by The Holydrug Couple\n",
      "空と虚 by sasanomaly\n",
      "None\n",
      "\n",
      "Please select one of the following options\n",
      "1. Get movie recommendations\n",
      "2. Get song recommendations\n",
      "3. Exit\n",
      "Please enter the song title you want a recommednation for\n",
      "You chose: Is This It by The Strokes\n",
      "Here are your recommendations:\n",
      "Destroy Everything You Touch by Ladytron\n",
      "Skiptracing by Mild High Club\n",
      "If I Could Find You (Eternity) by The Holydrug Couple\n",
      "Ishq Mubarak by Arijit Singh\n",
      "You and I Both by Jason Mraz\n",
      "Écoute Chérie by Vendredi sur Mer\n",
      "紅 by 告五人\n",
      "I Know A Place by Jay Reatard\n",
      "Touch, Peel And Stand by Days Of The New\n",
      "Kiri Ga Naikara by Fujii Kaze\n",
      "None\n",
      "\n",
      "Please select one of the following options\n",
      "1. Get movie recommendations\n",
      "2. Get song recommendations\n",
      "3. Exit\n",
      "Thank you for listening to my recommednations. Good bye :) \n"
     ]
    }
   ],
   "source": [
    "def get_movie_recommendations(title, movies):\n",
    "    if title in movies['title'].values: #check if movie exists in dataset\n",
    "        print(get_content_recommendations(title, cosine_similarity_matrix)) #if movie exists print recommendations\n",
    "    else:\n",
    "        print(\"So sorry! It seems as if the movie you are searching for is not available.\\n\")\n",
    "        print(\"Either the movie is not present in our database or was typed incorrectly.\\n\")\n",
    "        print(\"Make sure to spell the movie exactly as the title appears and try again.\")\n",
    "\n",
    "def get_song_recommendations(title, songs):\n",
    "    if title in songs['track_name'].values: #check if song exists in dataset\n",
    "        print(get_euclidean_recommendations(title, features, df, final_df)) #if song exists print recommendations\n",
    "    else:\n",
    "        print(\"So sorry! It seems as if the song you are trying to search for is not available.\\n\")\n",
    "        print(\"Either the song is not present in our database or was typed incorrectly.\\n\")\n",
    "        print(\"Make sure to spell the song exactly as it appears on Spotify and try again.\")\n",
    "\n",
    "def show_menu():\n",
    "    movies = df2 #load movie dataset\n",
    "    songs = df #load song dataset\n",
    "    \n",
    "    print(\"Hi! My name is S.a.M. I'm your Song and Movie recommendation system. I can't wait to get started.\")\n",
    "    \n",
    "    #Start an infinite loop to continually offer options until the user exists \n",
    "    while True:\n",
    "        print(\"\\nPlease select one of the following options\")\n",
    "        print(\"1. Get movie recommendations\")\n",
    "        print(\"2. Get song recommendations\")\n",
    "        print(\"3. Exit\")\n",
    "        choice = input(\"Please enter your choice: \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            print(\"Please enter the movie title you want a recommendation for:\")\n",
    "            title = input(\"Please enter the movie title you want a recommendations for: \")\n",
    "            get_movie_recommendations(title, movies)\n",
    "        elif choice == '2':\n",
    "            print(\"Please enter the song title you want a recommednation for\")\n",
    "            title = input(\"Please enter the song title you want a recommendation for: \")\n",
    "            get_song_recommendations(title, songs)\n",
    "        elif choice == '3':\n",
    "            print(\"Thank you for listening to my recommednations. Good bye :) \")\n",
    "            break\n",
    "        else:\n",
    "            print(\"I'm sorry but I didn't understand your answer. Please select 1 for movie recommendations. 2 for song recommednations and 3 to exit the program.\")\n",
    "\n",
    "show_menu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
